# entrenamiento_qdrant_bm25.py - Vectorstore Qdrant + BM25 con bge-small-en-v1.5
import os
import shutil
import uuid
import pickle
import time
import torch

# ‚úÖ VERIFICAR GPU
print("=" * 70)
print("üöÄ VERIFICANDO GPU PARA BGE-SMALL")
print("=" * 70)
if torch.cuda.is_available():
    print(f"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}")
    print(f"   Memoria: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    DEVICE = "cuda"
else:
    print("‚ö†Ô∏è  GPU NO detectada. Usando CPU...")
    DEVICE = "cpu"
print("=" * 70)

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain_text_splitters import RecursiveCharacterTextSplitter
import fitz
import pymupdf4llm

def clean_qdrant_db(collection_path="qdrant_db"):
    """Limpia directorio Qdrant existente"""
    if os.path.exists(collection_path):
        shutil.rmtree(collection_path)
    os.makedirs(collection_path, exist_ok=True)
    print(f"‚úÖ Directorio '{collection_path}' preparado")

def process_pdfs(pdf_folder="pdfs", chunk_size=1000, chunk_overlap=200):
    """Extrae texto de PDFs y divide en chunks"""
    if not os.path.exists(pdf_folder):
        print(f"‚ùå Carpeta '{pdf_folder}' no existe. Crea la carpeta y agrega tus PDFs.")
        return [], []
    
    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(".pdf")]
    if not pdf_files:
        print(f"‚ùå No hay PDFs en '{pdf_folder}'.")
        return [], []
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = []
    sources = []
    total_chunks = 0
    
    print(f"\nüìÑ Procesando {len(pdf_files)} PDFs...")
    print("=" * 60)
    
    for pdf_file in pdf_files:
        try:
            doc = fitz.open(os.path.join(pdf_folder, pdf_file))
            text = pymupdf4llm.to_markdown(doc)
            doc.close()
            
            if text.strip():
                file_chunks = splitter.split_text(text)
                valid_chunks = [c.strip() for c in file_chunks if len(c.strip()) > 100]
                chunks.extend(valid_chunks)
                sources.extend([pdf_file] * len(valid_chunks))
                total_chunks += len(valid_chunks)
                print(f"‚úÖ {pdf_file}: {len(valid_chunks)} chunks")
            else:
                print(f"‚ö†Ô∏è {pdf_file}: texto vac√≠o")
        except Exception as e:
            print(f"‚ùå Error en {pdf_file}: {str(e)[:80]}")
    
    print("=" * 60)
    print(f"üìä Total de chunks: {total_chunks}")
    return chunks, sources

def create_qdrant_index(chunks, sources, model_name="BAAI/bge-small-en-v1.5", collection_path="qdrant_db", device="cuda"):
    """
    Genera vectorstore Qdrant optimizado para GPU con bge-small-en-v1.5 (384d)
    
    ‚úÖ bge-small-en-v1.5 en GPU: ~2 minutos para 4000 chunks
    ‚úÖ bge-small-en-v1.5 en CPU: ~5 minutos para 4000 chunks
    """
    print(f"\nüß† Cargando modelo: {model_name} en {device.upper()}...")
    start_time = time.time()
    
    # ‚úÖ Cargar modelo en GPU (cuda) o CPU
    model = SentenceTransformer(model_name, device=device)
    load_time = time.time() - start_time
    print(f"‚úÖ Modelo cargado en {load_time:.1f}s")
    
    print(f"\nüîç Generando embeddings con {model_name} ({device.upper()})...")
    print("   ‚ö° Optimizado para GPU: batch_size=128")
    start_embed = time.time()
    
    # ‚úÖ Optimizado para GPU: batch_size mayor
    batch_size = 128 if device == "cuda" else 32
    embeddings = model.encode(
        chunks, 
        show_progress_bar=True, 
        normalize_embeddings=True,
        batch_size=batch_size,
        device=device
    )
    
    embed_time = time.time() - start_embed
    print(f"‚úÖ Embeddings generados: {embeddings.shape} en {embed_time/60:.1f} minutos")
    
    # Inicializar cliente Qdrant en modo local (disco)
    client = QdrantClient(path=collection_path)
    
    # Crear colecci√≥n
    client.create_collection(
        collection_name="acreditacion",
        vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE)
    )
    
    # ‚úÖ CORRECCI√ìN CR√çTICA: Usar PointStruct (no diccionarios)
    print("\nüíæ Guardando en Qdrant con PointStruct...")
    start_upsert = time.time()
    
    # Crear puntos con PointStruct
    points = []
    for i, (chunk, source, embedding) in enumerate(zip(chunks, sources, embeddings)):
        points.append(
            PointStruct(
                id=str(uuid.uuid4()),  # ‚úÖ ID como string
                vector=embedding.tolist(),
                payload={
                    "text": chunk,
                    "source": source,
                    "chunk_id": i
                }
            )
        )
    
    # Upsert en lotes
    batch_size = 256
    total_points = len(points)
    for i in range(0, total_points, batch_size):
        batch = points[i:i+batch_size]
        client.upsert(
            collection_name="acreditacion",
            points=batch
        )
        progress = min(i + batch_size, total_points)
        pct = progress / total_points * 100
        print(f"  ‚Üí {progress}/{total_points} puntos ({pct:.1f}%)")
    
    upsert_time = time.time() - start_upsert
    print(f"‚úÖ √çndice Qdrant guardado en '{collection_path}/' en {upsert_time:.1f}s")
    
    # ‚úÖ Exportar chunks para BM25
    bm25_data_path = os.path.join(collection_path, "bm25_data.pkl")
    with open(bm25_data_path, "wb") as f:
        pickle.dump({"chunks": chunks, "sources": sources}, f)
    print(f"‚úÖ Chunks exportados para BM25: {bm25_data_path}")
    
    total_time = time.time() - start_time
    print(f"\nüìö Modelo: {model_name} ({embeddings.shape[1]} dimensiones)")
    print(f"üìÑ Chunks indexados: {len(chunks)}")
    print(f"‚è±Ô∏è  Tiempo total: {total_time/60:.1f} minutos")
    return True

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("üöÄ GENERADOR DE VECTORSTORE QDRANT + BM25 CON BGE-SMALL (GPU)")
    print("=" * 70)
    print(f"\n‚öôÔ∏è  Configuraci√≥n:")
    print(f"   ‚Ä¢ Modelo: BAAI/bge-small-en-v1.5 (384 dimensiones)")
    print(f"   ‚Ä¢ Device: {DEVICE.upper()}")
    print(f"   ‚Ä¢ Batch size: {128 if DEVICE == 'cuda' else 32}")
    print(f"\n‚è±Ô∏è  Tiempo estimado:")
    print(f"   ‚Ä¢ GPU (RTX 3060+): 2-3 minutos para 4000 chunks")
    print(f"   ‚Ä¢ CPU: 5-7 minutos para 4000 chunks")
    print("=" * 70)
    
    clean_qdrant_db("qdrant_db")
    chunks, sources = process_pdfs("pdfs")
    
    if chunks:
        create_qdrant_index(
            chunks, 
            sources, 
            model_name="BAAI/bge-small-en-v1.5",  # ‚úÖ LIGERO Y R√ÅPIDO
            collection_path="qdrant_db",
            device=DEVICE
        )
        
        print("\n" + "=" * 70)
        print("‚úÖ ¬°VECTORSTORE CON BGE-SMALL CREADO EXITOSAMENTE!")
        print("=" * 70)
        print("\nüìå PR√ìXIMOS PASOS:")
        print("1. Comprime la carpeta qdrant_db:")
        print("   Compress-Archive -Path 'qdrant_db' -DestinationPath 'qdrant_db.zip'")
        print("2. Verifica tama√±o (< 50 MB):")
        print("   (Get-Item qdrant_db.zip).Length / 1MB")
        print("3. Sube qdrant_db.zip a GitHub")
        print("4. Usa app.py con RAG h√≠brido (BM25 + Qdrant + DeepSeek)")
        print("\nüéâ Calidad de embeddings: ‚≠ê‚≠ê‚≠ê‚≠ê (bge-small 384d)")
        print("   ‚úÖ 100% compatible con Streamlit Cloud (sin 'Killed')")
        print("=" * 70)
    else:
        print("\n‚ùå No se generaron chunks. Verifica tus PDFs en la carpeta 'pdfs/'.")